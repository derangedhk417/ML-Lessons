#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Neural Network Math / Training Process
\end_layout

\begin_layout Standard
\noindent
Consider the following neural network.
 This is essentially a single neuron, but it demonstrates the concepts just
 as well as a more complicated one.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename /Users/arobinson/Desktop/Screen Shot 2020-06-02 at 4.47.46 PM.png
	lyxscale 40
	scale 40

\end_inset


\end_layout

\begin_layout Standard
\noindent
This network can be written as
\begin_inset Formula 
\begin{align*}
f\left(x_{1},\,x_{2}\right) & =x_{1}w_{1}+x_{2}w_{2}+b\\
y & =\tanh\left[f\left(x_{1},\,x_{2}\right)\right]
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

Here I have chosen to apply a hyperbolic tangent activation function to
 the output of the neuron.
 Like I said, this choice is arbitrary and it is generally a good idea to
 try different activation functions when you are experimenting with a dataset.
 The training process will start by evaluating the function (neural network),
 for all of the inputs that you have available (your dataset).
 The following definitions are helpful,
\begin_inset Formula 
\begin{align}
\mathbf{X} & =\left\{ \mathbf{x}_{1},\,\mathbf{x}_{2},\ldots,\,\mathbf{x}_{N}\right\} \nonumber \\
\mathbf{x}_{i} & =\left\{ x_{i1},\,x_{i2}\right\} \nonumber \\
\tilde{\mathbf{Y}} & =\left\{ \mathbf{\tilde{y}}_{1},\,\tilde{\mathbf{y}}_{2},\,\ldots,\,\tilde{\mathbf{y}}_{N}\right\} \nonumber \\
\mathbf{\tilde{y}}_{i} & =\left\{ \tilde{y}_{i1}\right\} \nonumber \\
\mathbf{P} & =\left\{ p_{1},\,p_{2},\,\ldots,\,p_{M}\right\} =\left\{ w_{1},\,w_{2},\,b\right\} \label{eq:asfdsfas}\\
N & \equiv\mathrm{the\;number\;of\;datapoints\;in\;your\;dataset}\nonumber \\
M & \equiv\mathrm{the\;number\;of\;parameters\;in\;your\;model}\nonumber 
\end{align}

\end_inset


\begin_inset Newline newline
\end_inset

I have essentially defined the inputs and their corresponding known outputs
 as two dimensional arrays.
 It is completely equivalent to treat them as matrices, but I'm using the
 
\begin_inset Quotes eld
\end_inset

array
\begin_inset Quotes erd
\end_inset

 terminology here, because we are dealing code.
 Here, I use the tilde to denote the actual, known output for each input.
 The output of the neural network is
\begin_inset Formula 
\begin{align}
\mathbf{y}_{i} & =\left\{ \tanh\left[f\left(\mathbf{x}_{i}\right)\right]\right\} \label{eq:dgasdfadg}
\end{align}

\end_inset


\begin_inset Newline newline
\end_inset

The curly brackets denote that the output for each set of inputs is an array
 with a single element.
 This is because, in general, a neural network can have multiple outputs
 associated with each input.
 In this particular case it seems excessive, but in general it is necessary.
 At each iteration of the training, eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:dgasdfadg"

\end_inset

 will be evaluated for every input.
 I'll define the loss function for this network as the mean squared error
 (one of many options)
\begin_inset Formula 
\begin{align}
\sigma\left(\mathbf{Y},\,\tilde{\mathbf{Y}}\right) & =\cfrac{1}{N}\sum_{i=1}\left(\tilde{y}_{i}-y\right)^{2}\label{eq:sdfasdfs}
\end{align}

\end_inset


\begin_inset Newline newline
\end_inset

After eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:dgasdfadg"

\end_inset

 is calculated, eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sdfasdfs"

\end_inset

 will be calculated.
 PyTorch will then use chain rule and its internal computer algebra system
 to calculate the following,
\begin_inset Formula 
\begin{align}
\mathbf{G} & =\left\{ g_{1},\,g_{2},\,\ldots,\,g_{M}\right\} \nonumber \\
g_{i} & =\cfrac{\partial\sigma}{\partial p_{i}}\label{eq:fdgdfgd}
\end{align}

\end_inset


\begin_inset Newline newline
\end_inset

where each 
\begin_inset Formula $p_{i}$
\end_inset

 is one of the parameters (weight or bias) that defines the model.
 In practical models, the number of elements in the array in eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:asfdsfas"

\end_inset

 will be from 
\begin_inset Formula $10^{3}$
\end_inset

 to 
\begin_inset Formula $10^{5}$
\end_inset

.
 The most simple training algorithm available is known as Stochastic Gradient
 Descent (SGD).
 At each iteration of the training process, the algorithm will adjust the
 parameters of the model based on the following rule
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{align*}
p_{i}^{j+1} & =p_{i}^{j}-\alpha g_{i}\\
\alpha & \equiv\mathrm{learning\;rate}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

Here, the superscript on 
\begin_inset Formula $p$
\end_inset

 denotes the step at which 
\begin_inset Formula $p$
\end_inset

 takes that value.
 
\begin_inset Formula $p^{j+1}$
\end_inset

 would mean 
\begin_inset Quotes eld
\end_inset

the value of 
\begin_inset Formula $p$
\end_inset

 at the next step
\begin_inset Quotes erd
\end_inset

, not 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $p$
\end_inset

 to the power of 
\begin_inset Formula $j+1$
\end_inset


\begin_inset Quotes erd
\end_inset

.
 In this way, the algorithm will make incremental steps towards local minima
 in the neighborhood of the initial conditions of the neural network.
 This is an example of the most simple optimization algorithm available.
 The most important thing to understand is that almost all of them make
 use of gradients to make decisions about how to modify the parameters of
 the model with each step.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

This process has several major problems
\end_layout

\begin_layout Enumerate
If the learning rate is too high, it will overshoot local minima by taking
 steps that are too large.
\end_layout

\begin_layout Enumerate
If the learning rate is too low, it will only converge to local minima which
 may be very far from more optimal minima.
\end_layout

\begin_layout Enumerate
If the learning rate is too low it may take a very long time to converge
 to a local minima.
\end_layout

\begin_layout Enumerate
First order partial derivatives with respect to parameters are not a complete
 picture of the error surface of a function, see below.
\end_layout

\begin_layout Standard
The stochastic gradient descent method (much like almost all optimization
 methods) will modify every parameter by some amount at each step in the
 training process.
 By definition, the partial derivatives in eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:fdgdfgd"

\end_inset

 are the rates of change of the error with respect to a change in a specific
 parameter 
\series bold
with all other parameters held constant
\series default
.
 Despite this, the SGD algorithm changes all of the parameters by at least
 some small amount at each step.
 In principle, the following situation could occur, for a sufficiently complicat
ed problem
\begin_inset Formula 
\begin{align*}
\cfrac{\partial\sigma}{\partial p_{1}} & =-1\\
\cfrac{\partial\sigma}{\partial p_{2}} & =-1\\
\cfrac{\partial^{2}\sigma}{\partial p_{1}\partial p_{2}} & =1
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

In this case, the algorithm would take a step based on the first order gradients
 without knowing that a simultaneous step of both parameters will actually
 increase the error! A perfect algorithm would calculate all possible combinatio
ns of mixed partial derivatives.
 This isn't possible in practice though.
 For example, a network with 1000 parameter would have roughly 
\begin_inset Formula $1000^{1000}$
\end_inset

 possible combinations of mixed partial derivative (going up to arbitrary
 order).
 That number is so large that most calculators cant even compute it.
 In practice, most algorithms settle for only first order derivatives in
 order to save time.
 The moral of the story is that optimization algorithms are not perfect
 and sometimes they can fail outright.
 
\end_layout

\end_body
\end_document
